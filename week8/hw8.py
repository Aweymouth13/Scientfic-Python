# -*- coding: utf-8 -*-
"""hw8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ipd0UIRnhAoFY_2H_f1mfpQ1z09wTRX
"""

import numpy as np
from scipy.stats import chi2
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

!wget https://raw.githubusercontent.com/thaynecurrie/phys7943_fall2023/main/ScientificPythonNotes/_problemsets_github/week8/week8.md
!wget https://raw.githubusercontent.com/thaynecurrie/phys7943_fall2023/main/ScientificPythonNotes/_problemsets_github/week8/file.txt
!wget https://raw.githubusercontent.com/thaynecurrie/phys7943_fall2023/main/ScientificPythonNotes/_problemsets_github/week8/compilation.txt

"""## Problem 1
 1. Given 2 different models compute the p value for both model fits.
 3. Which model is consistant with a one and three sigma confidence limit?
"""

#we know if model has no free parameters v = N where v is dof, N is data points
# v= N - p where v is degrees of freedom, N is number of data points, and p is free parameters

#function for dof and p-value
def calc_dof_and_prob(N, p, x_square):
    v = N - p  # dof
    p_value = 1 - chi2.cdf(x_square, v)  # cdf
    return v, p_value

#set sigma confidence levels
confidence_1_sigma = 1 - 0.683
confidence_3_sigma = 1 - 0.997

#model 1
N_1, p_1, x_square_1 = 115, 2, 127
v_1, p_value_1 = calc_dof_and_prob(N_1, p_1, x_square_1)
print(f'model 1 has : {v_1} degrees of freedom')
print(f'model 1 p-value: {p_value_1:.3f}')
print(f'model 1 consistent at 1-sigma: {p_value_1 > confidence_1_sigma}')
print(f'model 1 consistent at 3-sigma: {p_value_1 > confidence_3_sigma}')

#model 2
N_2, p_2, x_square_2 = 15, 1, 26
v_2, p_value_2 = calc_dof_and_prob(N_2, p_2, x_square_2)
print(f'model 2 has : {v_2} degrees of freedom')
print(f'model 2 p-value: {p_value_2 : .3f}')
print(f'model 2 consistent at 1-sigma: {p_value_2 > confidence_1_sigma}')
print(f'model 2 consistent at 3-sigma: {p_value_2 > confidence_3_sigma}')

"""###Consistancy Analysis: P1

### Model 1
- **DOF**: 113  
  - Many more points compared to Model 2.
    
- **p-value**: 0.174  
  - About a 17% chance the data fits the model by random luck.
  
- **Consistency**:
  - Fails at 1-sigma but passes at 3-sigma.  
  - Not convincing at lower confidence limit, but at a high confidence limit (3-sigma) it fits well.

### Model 2
- **DOF**: 14  
  - Less data points than model 1.
  
- **p-value**: 0.026  
  - Only a 2.6% chance the data fits by random luck.
  
- **Consistency**:
  - Also fails at 1-sigma but passes at 3-sigma.  
  - Not convincing at lower confidence, but solid at 3-sigma.

### Summary
Both models aren't convincing at the 1-sigma level but they are solid at the 3-sigma level. Model 1 has a better p-value, making it slightly more trustworthy.

#Problem 2

###We can assume M is one, we can count N and we can record the reduced chi**2 values
##P value can be found using cdf once we have normal chi**2
"""

# Constants
M = 1 #assumed free params
N = 5 + 6 + 5 #total data points from each of the three discontinous plots

#reduced chi**2 for each star
chi_r = {
    'SIMP J1118': [1.904],
    'SIMP J0956': [0.893],
    'SIMP J1200': [2.499]
}

#normal chi-squared for each star
chi_squared = {}
p_values = {}


#to find chi**2 x^2 = x^2_reduced (N-M)
#to find P chi2.cdf(chi,v)

#iterate through each star
for star, reduced_chi_values in chi_r.items():
    chi_squared[star] = [x * (N - M) for x in reduced_chi_values]
    p_values[star] = [1 - chi2.cdf(chi, N - M) for chi in chi_squared[star]]

#show chi-squared values
print('Chi-squared values:', chi_squared)

#show p-values
print('P-values:', p_values)

#format print
print('\n')
print('Star      | Reduced Chi | Chi       | P-value')
print('----------|-------------|-----------|---------')

#loop thru dict
for star in chi_r.keys():
    for r_chi, chi, p in zip(chi_r[star], chi_squared[star], p_values[star]):
        print(f'{star:10} | {r_chi:11.2f} | {chi:9.2f} | {p:.3f}')

"""#Problem 3"""

#below you will find an altered version of your tpenalty function
#in the function call lower in the cell I will pass it the updated lamda/D value
#I have changed res_el by a factor of 1/2 since the problem statement
#says that it only uses half the field of view.
def tpenalty(sep=2,sigma=5,source=True):


 from scipy.stats import t
 from scipy.stats import norm

#change spatial resolution by factor of 1/2
#2(pi)*sep
 #res_el=2*np.pi*sep #if full view avaliable
 res_el = np.pi*sep


#Table 1/Figure 6 entries
 if source:

  ss_corr=np.sqrt(1+1/(res_el))
  penalty = t.ppf(norm.cdf(sigma),res_el-2)*ss_corr/sigma

 else:

  ss_corr=np.sqrt(1+1/res_el)
  penalty = t.ppf(norm.cdf(sigma),res_el-1)*ss_corr/sigma

 print('penalty is ',penalty)
 return penalty

#call tpenalty with new params
penalty_value = tpenalty(sep=2.5, sigma=5, source=False)
print('The contrast penalty at a distance of 2.5 Î»/D is:', penalty_value)
#note the value is linear at half the field of view penatly is 2*og

"""#Problem 4:

"""

#read the data
data = np.loadtxt('file.txt')
#extract x,y
x_data, y_data = data[:, 0], data[:, 1]

print('the data looks like: \n')
for row in data[:7]:
    print(row)
#now that i've seen it i suspect its poly of degree 2 w/ noise

#func for parabola
def parabolic(x, a, b, c):
    return a * x**2 + b * x + c

#curve fit - scipy
params, _ = curve_fit(parabolic, x_data, y_data)

# og data plot
plt.scatter(x_data, y_data, marker='x', color='g', label='Data')

#parabolic function
x_fit = np.linspace(min(x_data), max(x_data), 100)
y_fit_parabolic = parabolic(x_fit, *params)
plt.plot(x_fit, y_fit_parabolic, '--', color='orange', label='Parabolic Fit')

#axis labels
plt.xlabel('X', fontsize=14)
plt.ylabel('Y', fontsize=14)
plt.title("Do not include titles in Scientific Papers", fontsize=14)

#ledgend and export to vectorized figure
plt.legend()
plt.savefig('problem_4.pdf', format='PDF')
plt.show()

"""# Problem 5 / 6 Data Clean and Calcs"""

#read in data
data = np.genfromtxt('compilation.txt', delimiter='', dtype=str)

#flag seperate, not float
flag = data[:,4]

#column index mapping
index_map = {
    'jmag': 27, 'ejmag': 28,
    'hmag': 30, 'ehmag': 31,
    'kmag': 33, 'ekmag': 34,
    'lmag': 36, 'elmag': 37,
    'plx': 8, 'eplx': 9
}

#init vars
variables = {}

#loop : assign x_mag, ex_mag
for var_name, idx in index_map.items():
    variables[var_name] = data[:, idx].astype(float)

#filtering based on given conditions
lowg_indices = np.where(

    (flag == 'lowg') | (flag == 'young') | (flag == 'lowg,young') |

    (flag == 'ABDor,lowg,young') | (flag == 'Argus,lowg,young') | (flag == 'Columba,lowg,young') |

    (flag == 'TWA,lowg,young') | (flag == 'ScoCen,young') | (flag == 'TucHor,young') |

    (flag == 'Tuc-Hor,lowg,young') | (flag == 'plx-discrep,lowg'))


#init filtered vars
filtered_variables = {}


#apply filters loop : assign filter_xmag, filered_ex_mag
for var_name in index_map.keys():
    filtered_variables[f'filtered_{var_name}'] = variables[var_name][lowg_indices]

#calculations

#takes in two mags, returns the difference and their error prop (x-axis)
def calc_mag_diff(mag1, mag2, emag1, emag2):
    diff = mag1 - mag2
    diff_err = np.sqrt(emag1**2 + emag2**2)
    return diff, diff_err


#takes relative mag and error, plx's and returns absolute magnitude and error prop (Y-axis)
def calc_abs_mag(mag, plx, emag, eplx):
    abs_mag = mag - 5 * np.log10(1e2 / plx)
    abs_mag_err = np.sqrt(emag**2 + (5 * eplx / plx * (1 / np.log(10)))**2)
    return abs_mag, abs_mag_err

#would also turn this into a function if I was ever going to reuse this lol, will just call function for now

#J-H diff and its error
jmag_hmag_diff, jmag_hmag_diff_err = calc_mag_diff(variables['jmag'], variables['hmag'], variables['ejmag'], variables['ehmag'])
filtered_jmag_hmag_diff, filtered_jmag_hmag_diff_err = calc_mag_diff(filtered_variables['filtered_jmag'], filtered_variables['filtered_hmag'], filtered_variables['filtered_ejmag'], filtered_variables['filtered_ehmag'])

#J Mag and its error
abs_jmag, abs_jmag_err = calc_abs_mag(variables['jmag'], variables['plx'], variables['ejmag'], variables['eplx'])
filtered_abs_jmag, filtered_abs_jmag_err = calc_abs_mag(filtered_variables['filtered_jmag'], filtered_variables['filtered_plx'], filtered_variables['filtered_ejmag'], filtered_variables['filtered_eplx'])


#K-L diff and its error
kl_diff, kl_diff_err = calc_mag_diff(variables['kmag'], variables['lmag'], variables['ekmag'], variables['elmag'])
filtered_kl_diff, filtered_kl_diff_err = calc_mag_diff(filtered_variables['filtered_kmag'], filtered_variables['filtered_lmag'], filtered_variables['filtered_ekmag'], filtered_variables['filtered_elmag'])

#absolute magnitudes and their errors for Mh, Ml, Mk
abs_mh, abs_mh_err = calc_abs_mag(variables['hmag'], variables['plx'], variables['ehmag'], variables['eplx'])
filtered_abs_mh, filtered_abs_mh_err = calc_abs_mag(filtered_variables['filtered_hmag'], filtered_variables['filtered_plx'], filtered_variables['filtered_ehmag'], filtered_variables['filtered_eplx'])

abs_ml, abs_ml_err = calc_abs_mag(variables['lmag'], variables['plx'], variables['elmag'], variables['eplx'])
filtered_abs_ml, filtered_abs_ml_err = calc_abs_mag(filtered_variables['filtered_lmag'], filtered_variables['filtered_plx'], filtered_variables['filtered_elmag'], filtered_variables['filtered_eplx'])

abs_mk, abs_mk_err = calc_abs_mag(variables['kmag'], variables['plx'], variables['ekmag'], variables['eplx'])
filtered_abs_mk, filtered_abs_mk_err = calc_abs_mag(filtered_variables['filtered_kmag'], filtered_variables['filtered_plx'], filtered_variables['filtered_ekmag'], filtered_variables['filtered_eplx'])

"""## Problem 5 Plotting:"""

##resource used for color: https://imagecolorpicker.com/en
#i couldn't find a default 'Blue' etc that fit right!

#tuneable params
a_full = 1 #alpha full data
a_filtered = .85
mcf = '#1e77b4'  #full marker color
mecf = 'black'  #full marker edge color
mcfilt = '#ec9e09'  #filtered marker color
mecfilt = 'black'  #filtered marker edge color
ecf = '#ff9e4a'  #full error color
ecfilt = '#889f39'  #filtered error color
ms = 12  #marker size
afs = 14  #axis font size
lfs = 8  #legend font size
#maybe add bold ledgend or different font?

#fig axes call
fig, ax = plt.subplots(figsize=(6, 8))

#full dataset plot w/ params
ax.errorbar(jmag_hmag_diff, abs_jmag, xerr=jmag_hmag_diff_err, yerr=abs_jmag_err, fmt='o', c=mcf, markeredgecolor=mecf, ecolor=ecf, alpha=a_full, label='Field MLT Dwarfs', markersize=ms)

#filtered dataset w/ params
ax.errorbar(filtered_jmag_hmag_diff, filtered_abs_jmag, xerr=filtered_jmag_hmag_diff_err, yerr=filtered_abs_jmag_err, fmt='o', c=mcfilt, markeredgecolor=mecfilt, ecolor=ecfilt, alpha=a_filtered, label='Young/Low-Gravity MLT Dwarfs', markersize=ms)

#axes and ledgend
ax.set_xlabel('J - H', fontsize=afs)
ax.set_ylabel(r'$M_J$', fontsize=afs)
ax.legend(loc='upper left', fontsize=lfs)

#limits
ax.set_ylim(20, 8) #invert
ax.set_xlim(-2.5, 2.5)

#show
plt.savefig('plot_5.pdf',format='pdf')
plt.show()

"""##Problem 6 Brute Force"""

#same tuneable params as above

#2z2 subplot
fig, axs = plt.subplots(2, 2, figsize=(12, 16))

#xlimits and ticks
axs[0, 0].set_xlim(-2.5, 2.5)
axs[0, 0].set_xticks(range(-2, 3, 1))

axs[0, 1].set_xlim(-1, 4)
axs[0, 1].set_xticks(range(-1, 5, 1))

axs[1, 0].set_xlim(-2.5, 2.5)
axs[1, 0].set_xticks(range(-2, 3, 1))

axs[1, 1].set_xlim(-1, 4)
axs[1, 1].set_xticks(range(-1, 5, 1))

#top left: J - H vs MJ
axs[0, 0].errorbar(jmag_hmag_diff, abs_jmag, xerr=jmag_hmag_diff_err, yerr=abs_jmag_err, fmt='o', c=mcf, markeredgecolor=mecf, ecolor=ecf, alpha=a_full, markersize=ms)
axs[0, 0].errorbar(filtered_jmag_hmag_diff, filtered_abs_jmag, xerr=filtered_jmag_hmag_diff_err, yerr=filtered_abs_jmag_err, fmt='o', c=mcfilt, markeredgecolor=mecfilt, ecolor=ecfilt, alpha=a_filtered, markersize=ms)
axs[0, 0].set_xlabel('J - H', fontsize=afs)
axs[0, 0].set_ylabel(r'$M_J$', fontsize=afs)

#top right: K - L vs MK
axs[0, 1].errorbar(kl_diff, abs_mk, xerr=kl_diff_err, yerr=abs_mk_err, fmt='o', c=mcf, markeredgecolor=mecf, ecolor=ecf, alpha=a_full, markersize=ms)
axs[0, 1].errorbar(filtered_kl_diff, filtered_abs_mk, xerr=filtered_kl_diff_err, yerr=filtered_abs_mk_err, fmt='o', c=mcfilt, markeredgecolor=mecfilt, ecolor=ecfilt, alpha=a_filtered, markersize=ms)
axs[0, 1].set_xlabel('K - L', fontsize=afs)
axs[0, 1].set_ylabel(r'$M_K$', fontsize=afs)

#bottom left: J - H vs MH
axs[1, 0].errorbar(jmag_hmag_diff, abs_mh, xerr=jmag_hmag_diff_err, yerr=abs_mh_err, fmt='o', c=mcf, markeredgecolor=mecf, ecolor=ecf, alpha=a_full, markersize=ms)
axs[1, 0].errorbar(filtered_jmag_hmag_diff, filtered_abs_mh, xerr=filtered_jmag_hmag_diff_err, yerr=filtered_abs_mh_err, fmt='o', c=mcfilt, markeredgecolor=mecfilt, ecolor=ecfilt, alpha=a_filtered, markersize=ms)
axs[1, 0].set_xlabel('J - H', fontsize=afs)
axs[1, 0].set_ylabel(r'$M_H$', fontsize=afs)

#bottom right: K - L vs ML
axs[1, 1].errorbar(kl_diff, abs_ml, xerr=kl_diff_err, yerr=abs_ml_err, fmt='o', c=mcf, markeredgecolor=mecf, ecolor=ecf, alpha=a_full, markersize=ms)
axs[1, 1].errorbar(filtered_kl_diff, filtered_abs_ml, xerr=filtered_kl_diff_err, yerr=filtered_abs_ml_err, fmt='o', c=mcfilt, markeredgecolor=mecfilt, ecolor=ecfilt, alpha=a_filtered, markersize=ms)
axs[1, 1].set_xlabel('K - L', fontsize=afs)
axs[1, 1].set_ylabel(r'$M_L$', fontsize=afs)

#all share common y lims
for ax in axs.flat:
    ax.set_ylim(20, 8)

#show/save
plt.tight_layout()
plt.savefig('question_6.pdf', format='pdf')
plt.show()

"""## Problem 6 : Elegant :)"""

#same tuneable params as above
fig, axs = plt.subplots(2, 2, figsize=(12, 16))

#set x, xf, y,yf,xe,fxr,yr,fyr
x_data = [jmag_hmag_diff, kl_diff, jmag_hmag_diff, kl_diff]
filtered_x_data = [filtered_jmag_hmag_diff, filtered_kl_diff, filtered_jmag_hmag_diff, filtered_kl_diff]
y_data = [abs_jmag, abs_mk, abs_mh, abs_ml]
filtered_y_data = [filtered_abs_jmag, filtered_abs_mk, filtered_abs_mh, filtered_abs_ml]
x_err_data = [jmag_hmag_diff_err, kl_diff_err, jmag_hmag_diff_err, kl_diff_err]
filtered_x_err_data = [filtered_jmag_hmag_diff_err, filtered_kl_diff_err, filtered_jmag_hmag_diff_err, filtered_kl_diff_err]
y_err_data = [abs_jmag_err, abs_mk_err, abs_mh_err, abs_ml_err]
filtered_y_err_data = [filtered_abs_jmag_err, filtered_abs_mk_err, filtered_abs_mh_err, filtered_abs_ml_err]

#labels
x_labels = ['J - H', 'K - L', 'J - H', 'K - L']
y_labels = [r'$M_J$', r'$M_K$', r'$M_H$', r'$M_L$']

#loop through the flattened axs array and the data lists
for ax, x, y, x_err, y_err, fx, fy, fx_err, fy_err, xl, yl in zip(axs.flat, x_data, y_data, x_err_data, y_err_data, filtered_x_data, filtered_y_data, filtered_x_err_data, filtered_y_err_data, x_labels, y_labels):
    ax.errorbar(x, y, xerr=x_err, yerr=y_err, fmt='o', c=mcf, markeredgecolor=mecf, ecolor=ecf, alpha=a_full, markersize=ms)
    ax.errorbar(fx, fy, xerr=fx_err, yerr=fy_err, fmt='o', c=mcfilt, markeredgecolor=mecfilt, ecolor=ecfilt, alpha=a_filtered, markersize=ms)
    ax.set_xlabel(xl, fontsize=afs)
    ax.set_ylabel(yl, fontsize=afs)
    ax.set_ylim(20, 8)

#loop different x's
limits = [(-2.5, 2.5), (-1, 4), (-2.5, 2.5), (-1, 4)]
ticks = [range(-2, 3, 1), range(-1, 5, 1), range(-2, 3, 1), range(-1, 5, 1)]

for ax, lim, tick in zip(axs.flatten(), limits, ticks):
    ax.set_xlim(lim)
    ax.set_xticks(tick)

plt.tight_layout()
plt.savefig('question6_but_better.pdf', format='pdf')
plt.show()

"""# Problem 7: Update

## Overview
- We are creating an N-body simulation, tracking physical variables, and doing an analysis on them.

## Libraries Used
- Matplotlib
- Pandas
- Numpy
- Combinations (native)
- Astropy
- Scipy

## Progress
- Current Status is 65% complete.
  - We feel that we have a good (enough) model for a relatively short timescale.
  - Decided not to add in EM effects, only considering gravity.

## Issues
- If the simulation goes on long enough, we see the total energy of the system not stay constant.
  - This likely comes from how we are calculating the differential.

## To-Do
- Still need to do the analysis
  - Have come up with interesting metrics to show off the system

## Plots
- No Plots to share at this time.

"""
# -*- coding: utf-8 -*-
"""hw8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ipd0UIRnhAoFY_2H_f1mfpQ1z09wTRX
"""

import numpy as np
from scipy.stats import chi2
from scipy.optimize import curve_fit
import matplotlib.pyplot as plt

!wget https://raw.githubusercontent.com/thaynecurrie/phys7943_fall2023/main/ScientificPythonNotes/_problemsets_github/week8/week8.md
!wget https://raw.githubusercontent.com/thaynecurrie/phys7943_fall2023/main/ScientificPythonNotes/_problemsets_github/week8/file.txt

"""## Problem 1
 1. Given 2 different models compute the p value for both model fits.
 3. Which model is consistant with a one and three sigma confidence limit?
"""

#we know if model has no free parameters v = N where v is dof, N is data points
# v= N - p where v is degrees of freedom, N is number of data points, and p is free parameters

#function for dof and p-value
def calc_dof_and_prob(N, p, x_square):
    v = N - p  # dof
    p_value = 1 - chi2.cdf(x_square, v)  # cdf
    return v, p_value

#set sigma confidence levels
confidence_1_sigma = 1 - 0.683
confidence_3_sigma = 1 - 0.997

#model 1
N_1, p_1, x_square_1 = 115, 2, 127
v_1, p_value_1 = calc_dof_and_prob(N_1, p_1, x_square_1)
print(f'model 1 has : {v_1} degrees of freedom')
print(f'model 1 p-value: {p_value_1:.3f}')
print(f'model 1 consistent at 1-sigma: {p_value_1 > confidence_1_sigma}')
print(f'model 1 consistent at 3-sigma: {p_value_1 > confidence_3_sigma}')

#model 2
N_2, p_2, x_square_2 = 15, 1, 26
v_2, p_value_2 = calc_dof_and_prob(N_2, p_2, x_square_2)
print(f'model 2 has : {v_2} degrees of freedom')
print(f'model 2 p-value: {p_value_2 : .3f}')
print(f'model 2 consistent at 1-sigma: {p_value_2 > confidence_1_sigma}')
print(f'model 2 consistent at 3-sigma: {p_value_2 > confidence_3_sigma}')

"""###Consistancy Analysis: P1

### Model 1
- **DOF**: 113  
  - Many more points compared to Model 2.
    
- **p-value**: 0.174  
  - About a 17% chance the data fits the model by random luck.
  
- **Consistency**:
  - Fails at 1-sigma but passes at 3-sigma.  
  - Not convincing at lower confidence limit, but at a high confidence limit (3-sigma) it fits well.

### Model 2
- **DOF**: 14  
  - Less data points than model 1.
  
- **p-value**: 0.026  
  - Only a 2.6% chance the data fits by random luck.
  
- **Consistency**:
  - Also fails at 1-sigma but passes at 3-sigma.  
  - Not convincing at lower confidence, but solid at 3-sigma.

### Summary
Both models aren't convincing at the 1-sigma level but they are solid at the 3-sigma level. Model 1 has a better p-value, making it slightly more trustworthy.

#Problem 2

###We can assume M is one, we can count N and we can record the reduced chi**2 values
##P value can be found using cdf once we have normal chi**2
"""

# Constants
M = 1 #assumed free params
N = 5 + 6 + 5 #total data points from each of the three discontinous plots

#reduced chi**2 for each star
chi_r = {
    'SIMP J1118': [1.904],
    'SIMP J0956': [0.893],
    'SIMP J1200': [2.499]
}

#normal chi-squared for each star
chi_squared = {}
p_values = {}


#to find chi**2 x^2 = x^2_reduced (N-M)
#to find P chi2.cdf(chi,v)

#iterate through each star
for star, reduced_chi_values in chi_r.items():
    chi_squared[star] = [x * (N - M) for x in reduced_chi_values]
    p_values[star] = [1 - chi2.cdf(chi, N - M) for chi in chi_squared[star]]

#show chi-squared values
print('Chi-squared values:', chi_squared)

#show p-values
print('P-values:', p_values)

#format print
print('\n')
print('Star      | Reduced Chi | Chi       | P-value')
print('----------|-------------|-----------|---------')

#loop thru dict
for star in chi_r.keys():
    for r_chi, chi, p in zip(chi_r[star], chi_squared[star], p_values[star]):
        print(f'{star:10} | {r_chi:11.2f} | {chi:9.2f} | {p:.3f}')

"""#Problem 3"""

#below you will find an altered version of your tpenalty function
#in the function call lower in the cell I will pass it the updated lamda/D value
#I have changed res_el by a factor of 1/2 since the problem statement
#says that it only uses half the field of view.
def tpenalty(sep=2,sigma=5,source=True):


 from scipy.stats import t
 from scipy.stats import norm

#change spatial resolution by factor of 1/2
#2(pi)*sep
 #res_el=2*np.pi*sep #if full view avaliable
 res_el = np.pi*sep


#Table 1/Figure 6 entries
 if source:

  ss_corr=np.sqrt(1+1/(res_el))
  penalty = t.ppf(norm.cdf(sigma),res_el-2)*ss_corr/sigma

 else:

  ss_corr=np.sqrt(1+1/res_el)
  penalty = t.ppf(norm.cdf(sigma),res_el-1)*ss_corr/sigma

 print('penalty is ',penalty)
 return penalty

#call tpenalty with new params
penalty_value = tpenalty(sep=2.5, sigma=5, source=False)
print('The contrast penalty at a distance of 2.5 Î»/D is:', penalty_value)
#note the value is linear at half the field of view penatly is 2*og

"""#Problem 4:

"""

#read the data
data = np.loadtxt('file.txt')
#extract x,y
x_data, y_data = data[:, 0], data[:, 1]

print('the data looks like: \n')
for row in data[:7]:
    print(row)
#now that i've seen it i suspect its poly of degree 2 w/ noise

#func for parabola
def parabolic(x, a, b, c):
    return a * x**2 + b * x + c

#curve fit - scipy
params, _ = curve_fit(parabolic, x_data, y_data)

# og data plot
plt.scatter(x_data, y_data, marker='x', color='g', label='Data')

#parabolic function
x_fit = np.linspace(min(x_data), max(x_data), 100)
y_fit_parabolic = parabolic(x_fit, *params)
plt.plot(x_fit, y_fit_parabolic, '--', color='orange', label='Parabolic Fit')

#axis labels
plt.xlabel('X', fontsize=14)
plt.ylabel('Y', fontsize=14)
plt.title("Do not include titles in Scientific Papers", fontsize=14)

#ledgend and export to vectorized figure
plt.legend()
plt.savefig('problem_4.pdf', format='PDF')
plt.show()